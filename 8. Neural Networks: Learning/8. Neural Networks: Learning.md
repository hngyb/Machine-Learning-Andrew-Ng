# 8. Neural Networks: Learning
# Cost Function and Backpropagation
## 8.1 Cost Function   
  ![](https://wikidocs.net/images/page/4261/nn401.PNG "")   
  ![2](https://user-images.githubusercontent.com/68726615/90242572-4099ae80-de68-11ea-8290-eb881921289c.png "")   
- Binary classification
    - y = 0 or 1
    - 1 output unit
- Multi-class classification (K classes)
    - k output units
    ![](https://wikidocs.net/images/page/4261/nn402.PNG "")
### Cost function
- Logistic regression의 cost function   
    ![1](https://user-images.githubusercontent.com/68726615/90242575-41324500-de68-11ea-878c-f10b8d0860f1.png "")   
- Neural Network의 cost function   
    ![3](https://user-images.githubusercontent.com/68726615/90242571-40011800-de68-11ea-8363-961ac11d9454.png "")   
    - mulitple output nodes를 고려하기 위해 몇 개의 nested summation이 추가됨.
    - 방정식의 첫 번재 부분인 대괄호([]) 사이에는, output node 개수만큼 반복되는 nested summation이 추가됨.
    - Regularization 항에는 복수의 Θ행렬을 고려해야 함.
    - 현재 Θ 행렬의 열 개수는 현재 layer의 node 개수 (bias unit 포함)와 같음.
    - Θ 행렬의 행 개수는 다음 layer의 node 개수 (bias unit 제외)와 같음.
    - the double sum simply adds up the logistic regression costs calculated for each cell in the output layer
    - the triple sum simply adds up the squares of all the individual Θs in the entire network.
    - the i in the triple sum does __not__ refer to training example i
## 8.2 Backpropagation Algorithm
### Gradient computation
- cost function을 최소화하는 최적의 parameter를 찾아야 함.
- 이를 위해, cost function J(Θ)를 정의하고, 편미분을 계산해야 함.
    ![](https://wikidocs.net/images/page/4262/nn501.PNG "")
    ![](https://wikidocs.net/images/page/4262/nn502.PNG "")
### Gradient computation: Backpropagation algorithm
- BP는 output layer에서 시작함.
- 마지막 결과의 'error'를 먼저 구하고, 해당 error 값을 이용하여 각각의 node에서의 error를 계산함.   
    ![4](https://user-images.githubusercontent.com/68726615/90242570-40011800-de68-11ea-8bf3-cb2e7bcffbc8.png "")   
    ![](https://wikidocs.net/images/page/4262/nn504.PNG "")
- 마지막 layerd의 δ^(L) = a^(L) - y
- 이전 layer δ^(l)는   
    ![5](https://user-images.githubusercontent.com/68726615/90242567-3f688180-de68-11ea-93a0-7f62e8bf674e.png "")   
-  .∗ 연산은 벡터의 element-by-element 곱셈을 뜻함.
- g′(z)는 z에 대해 g(z)를 미분한 함수.
- g′(z^(l))=a^(l).∗(1−a^(l))로 나타낼 수 있음.   
    ![6](https://user-images.githubusercontent.com/68726615/90242561-3d9ebe00-de68-11ea-8166-ec4c1c27a293.png "")   
### Backpropagation algorithm
  ![](https://wikidocs.net/images/page/4262/nn507.PNG "")
- 잘 이해를 못함...
## 8.3 Backpropagation Intuition
### Forward Propagation
  ![](https://wikidocs.net/images/page/4279/nn601.PNG "")   
### What is backpropagation doing?
  ![](https://wikidocs.net/images/page/4279/nn603.PNG "")
  ![](https://wikidocs.net/images/page/4279/nn604.PNG "")
  ![](https://wikidocs.net/images/page/4279/nn605.PNG "")
  ![](https://wikidocs.net/images/page/4279/nn606.PNG "")
# Backpropagation in Practice
## 8.4 Implementation Note: Unrolling Parameters   
  ![](https://wikidocs.net/images/page/4265/nn701.PNG "")   
## 8.5 Gradient Checking
- Gradient Checking은 backpropagation이 제대로 작동하고 있는지 확인하는 방법임.
  ![](https://wikidocs.net/images/page/4265/nn702.PNG "")   
  ![7](https://user-images.githubusercontent.com/68726615/90242549-3aa3cd80-de68-11ea-9bfd-4e66a98112f1.png "")   
- multiple theta matrices에서도, 각각의 theta에 적용하여 approximate.   
  ![](https://wikidocs.net/images/page/4265/nn703.PNG "")   
- epsilon의 크기가 작아야 정상적으로 작동함. 하지만 너무 작으면, numerical problems 발생할 수 있음.
- Gradient Checking을 통해 backpropagation 알고리즘이 정상적으로 작동하는 것을 확인한 후에는, classifier를 train하기 전에 gradient checking을 disable해야 함. 그러지 않으면 굉장히 느려짐.
## 8.6 Random Initialization
- 모든 theta를 '0'으로 초기화하는 것은 neural networks에서는 작동하지 않음.
  ![](https://wikidocs.net/images/page/4265/nn801.PNG "")
- 한 번 weight를 업데이트할 때마다 같은 입력단에서 나온 weight들에는 같은 값이 들어감.
### Random Initialization: Symmetric Breaking
- Initialize each Θij^(l) to a random value in [−ϵ,ϵ]
## 8.7 Putting It Together
- 먼저 network architecture를 선택함.
  - Number of input units = dimension of features x^(i)
  - Number of ouput units = number of classes
  - Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)
  - Defaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.
  ### Training a Neural Network
  1. Randomly initialize the weights.
  2. Implement forward propagation to get hΘ(x^(i)) for any x^(i).
  3. Implement the cost function.
  4. Implement backpropagation to compute partial derivatives.
  5. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.
  6. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.   
    ![](https://user-images.githubusercontent.com/68726615/93430990-7cf47a80-f8fe-11ea-8e18-86d86bf52410.png)   
# Application of Neural Networks
## 8.8 Autonomous Driving