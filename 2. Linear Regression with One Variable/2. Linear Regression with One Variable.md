# 2. Linear Regression with One Variable

# Model and Cost Function

## 2.1 Model Representation

- 지도학습에서 우리가 갖고 있는  데이터셋을 학습세트(training set)이라고 함.
- 강의에서 사용할 표기법(notation) 정의:
    - **m**: # training examples
    - **x's**: "input" variable, or features
    - **y's**: "output" variable, or "target" variable
    - **(x,y)**: one training example
    - **(x(i), y(i))**: i-th training example
- __H(Hypothesis)__ 란, input과 output의 관계를 나타내는 함수. H is map from x to y.

    ![](https://user-images.githubusercontent.com/68726615/88515452-ae4f7900-d026-11ea-925c-d625efd35c51.png)

- Hypothesis라는 말이 가정 적합한 단어는 아니지만, 과거부터 기계학습에서 전문용어로 사용됨.
- **hθ(x)=θ0+θ1x**

    ![https://wikidocs.net/images/page/4213/linreg202.PNG](https://wikidocs.net/images/page/4213/linreg202.PNG "")

    - x의 선형함수인 y를 예측.
    - 하나의 변량을 갖고 있는 선형회귀(Linear regression with one variable).
    - 이 모형의 다른 이름은 **단일변량 선형회귀(Univariate linear regression)**.

## 2.2 Cost Function

> **Hypothesis: hθ(x)=θ0+θ1x**

- **θi**들을 parameter라고 함.
- 어떻게 parameter의 값을 선택할 것인가의 문제.

![https://wikidocs.net/images/page/4213/linreg301.PNG](https://wikidocs.net/images/page/4213/linreg301.PNG "")

![https://wikidocs.net/images/page/4213/linreg302.PNG](https://wikidocs.net/images/page/4213/linreg302.PNG "")

- **h(X)** 의 값이 y에 가장 가깝도록 parameter를 설정. 이를 위해 **Cost function**을 이용.

![https://user-images.githubusercontent.com/68726615/88515425-a98ac500-d026-11ea-8db2-543de3f5efff.png](https://user-images.githubusercontent.com/68726615/88515425-a98ac500-d026-11ea-8db2-543de3f5efff.png "")

- **J(θ0,θ1)** 를 Cost function이라고 하며, 이를 최소화 하는 것이 목표. **Minimize J(θ0,θ1).**
- 위의 Cost function은 **mean-squared-error(MSE).** 다만 평균이라면 data 개수인 m으로 나누어야하는데, 2m으로 나눈 이유는 계산상의 편의를 위한 것.

## 2.3 Cost Function Intuition 1

Cost function J를 더욱 시각화하기 위해서, hypothesis를 간소화함 (**θ0 = 0**으로 설정).

![https://wikidocs.net/images/page/4213/linreg401.PNG](https://wikidocs.net/images/page/4213/linreg401.PNG "")

![https://wikidocs.net/images/page/4213/linreg402.PNG](https://wikidocs.net/images/page/4213/linreg402.PNG "")

- Training data: (1,1), (2,2), (3,)
- **θ1 = 1**인 경우, **J(1) = 0**. 주어진 데이터에 완벽하게 맞아 떨어지므로 비용함수가 0
- **θ1**에 여러 값을 대입하여 **J(θ1)**의 그래프를그리게 되면 위와 같이 포물선 형태가 만들어짐.
- 이 그래프가 최소값을 갖도록 하는 **θ1 = 1**이 최적의 값이 됨.

## 2.4 Cost Function Intuition 2

**θ0** 값이 존재하는 Cost Function을 시각화. 이때 **Contour plots (Contour figures)** 을 사용하게 됨.

![](https://user-images.githubusercontent.com/68726615/88515430-aa235b80-d026-11ea-95e7-ea785956951a.png)

![](https://user-images.githubusercontent.com/68726615/88515432-aabbf200-d026-11ea-9096-9b8ff6320be0.png)

- 같은 색의 원 안에 있는 점들은 동일한 Cost Function의 값을 가지게 됨.
- 가장 안쪽에 있는 원이 가장 최적화된 Cost Function의 값.
- Θ0의 값과 이 Θ1의 값이 만나는 점이 h(x)값.

# Parameter Learning

## 2.5 Gradient Descent

- Cost Function J의 최소값을 구하기 위해 **"경사 하강(Gradient Descent)"** 알고리즘을 사용.
- **Gradient Descent Outline**
    - Start with some θ0, θ1
    - Keep changing θ0, θ1 to reduce J(θ0,θ1) until we hopefully end up at a minimum

        ![](https://user-images.githubusercontent.com/68726615/88515433-aabbf200-d026-11ea-97a9-968464074804.png)

- **Gradient Descent Algorithm**
![https://wikidocs.net/images/page/7635/linreg601.PNG](https://wikidocs.net/images/page/7635/linreg601.PNG "")
    - repeat until convergence:
![https://user-images.githubusercontent.com/68726615/88515436-abed1f00-d026-11ea-9be2-be20ac33876b.png](https://user-images.githubusercontent.com/68726615/88515436-abed1f00-d026-11ea-9be2-be20ac33876b.png "")
    - **:=** "assignment" operator
    - **α** "learning rate"
    - **j** "feature index number, should be updated simultaneously
    - **At each iteration j, one should simultaneously update the parameters θ0, θ1, ..., θn.** parameter들을 동시에 업데이트해야 함.

## 2.6 Gradient Descent Intuition

Repeat until convergence:

![https://user-images.githubusercontent.com/68726615/88515436-abed1f00-d026-11ea-9be2-be20ac33876b.png](https://user-images.githubusercontent.com/68726615/88515436-abed1f00-d026-11ea-9be2-be20ac33876b.png "")

- α는 learning rate으로, 얼마나 많은 값이 변하는가에 대한 변수, parameter J값이 영향을 받음.
- 편미분항은 다음에 이동할 방향과 크기를 결정 (Tangent값을 구함)

    ![https://user-images.githubusercontent.com/68726615/88515441-ac85b580-d026-11ea-9a47-aa4ea7acfcd1.png](https://user-images.githubusercontent.com/68726615/88515441-ac85b580-d026-11ea-9a47-aa4ea7acfcd1.png "")

![https://wikidocs.net/images/page/7635/linreg603.PNG](https://wikidocs.net/images/page/7635/linreg603.PNG "")

- 위의 예시에서 편미분항이 양수이면 왼쪽으로, 음수이면 오른쪽으로 움직여 최소값에 가까워는 것을 확인할 수 있음.

![](https://user-images.githubusercontent.com/68726615/88515443-ad1e4c00-d026-11ea-9026-427406cdf69d.png)

- α가 너무 작으면 경사 하강이 느려지고, α가 너무 크면, 최소값에 수렴하지 못하거나 발산하는 문제가 발생할 수 있음.

![https://wikidocs.net/images/page/7635/linreg702.PNG](https://wikidocs.net/images/page/7635/linreg702.PNG "")

- Gradient descent가 local optima에 이르면 편미분항이 0이 되므로 더 이상 업데이트 되지 않음.
- learning rate α가 고정되어 있더라도, Gradient descent는 local minimum으로 수렴할 수 있음.
    - local minimum에 다가갈수록, gradient descent의 크기가 작아지기 때문에 α를 조절하지 않아도 됨.

    ![](https://user-images.githubusercontent.com/68726615/88515448-adb6e280-d026-11ea-9154-647593e4462a.png)

## 2.7 Gradient Descent For Linear Regression

- Gradient Descent를 선형회귀에 적용시키면, (Linear Regression에서 정의한 Cost function을 Gradient Descent algorithm의 J에 대입한 후, 미분)

![https://user-images.githubusercontent.com/68726615/88515451-adb6e280-d026-11ea-9700-e46e98fe6dcf.png](https://user-images.githubusercontent.com/68726615/88515451-adb6e280-d026-11ea-9700-e46e98fe6dcf.png "")

- **Linear Regression의 Cost Function은 local optima없이 global optima만 가지고 있는 볼록함수(Convex function)임.**
- Linear Regression의 Gradient Descent는 항상 global optima값을 향하게 됨.
- **"Batch gradient descent"** 이라고도 불림. Gradient descent의 매 단계에서 모든 training example을 사용함. This method looks at every example in the entire training set on every step.
