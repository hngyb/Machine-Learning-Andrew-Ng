# 5. Logistic Regression
# Classification and Representation
## 5.1 Classification
- Classification의 예시
  - Email: Spam / Not Spam
  - Online Transactions: Fraudulent (Yes / No)
  - Tumor: Malignant / Benign
- Binary classficiation problem: y의 값이 오직 두 개의 값, 0 과 1을 가지는 분류 문제.   

  ![0](https://user-images.githubusercontent.com/68726615/89259961-576d2380-d666-11ea-89cb-75bead9a6fba.png "")
### Linear Regression for Classification
- 종양의 크기로 악성인지 양성인지 판별하는 문제에 Linear Regression를 적용하면,   
  ![](https://wikidocs.net/images/page/4267/logreg102.PNG "")   
  ![1](https://user-images.githubusercontent.com/68726615/89259962-576d2380-d666-11ea-9848-d73911474b6c.png "")   
- 하지만, 파란 선을 기준으로 threshold를 잡을 경우 classification이 제대로 되지 않음.   
  ![](https://wikidocs.net/images/page/4267/logreg104.PNG "")   
- Classification에서 y의 값은 오직 0 또는 1이지만, linear regression은 1보다 크거나 0보다 작은 값을 가질 수 있음.
- 따라서, 0과 1사이의 값만 내보내는 Logistic Regression을 통해 Classification 문제를 해결하려 함.

## 5.2 Hypothesis Representation
### Logistic Regression Model
  - Logistic Regression Model은 0≤hθ(x)≤1 를 만족해야 함.
  - "Sigmoid Function" 또는 "Logistic function" 이라고 불리는 g(z)함수를 이용.   
    ![](https://wikidocs.net/images/page/4287/logreg201.PNG "")   
  - Sigmoid function은 다음과 같음.   
    ![](https://user-images.githubusercontent.com/68726615/93429785-a6aca200-f8fc-11ea-89c2-4f11c484e159.png) 
### Interpretation of Hypothesis Output
- Hypotheis function hθ(x)는 x 값이 주어졌을 때, y = 1 일 확률임. Estimated probability that y = 1 on input x.
  ![2](https://user-images.githubusercontent.com/68726615/89259963-5805ba00-d666-11ea-8a32-ed5ced82de7f.png "")
  - probability that y = 1, given x, parameterized by θ
- E.g. hθ(x)=0.7 이란, output이 1일 확률이 70%이며 output이 0일 확률은 30%라는 뜻.
## 5.3 Decision Boundary
  ![](https://wikidocs.net/images/page/4288/logreg301.PNG "")   
- Hypothesis function hθ(x)가 0.5보다 크거나 같으면 y = 1, 0.5보다 작으면 y = 0으로 predict.   
  ![3](https://user-images.githubusercontent.com/68726615/89259964-5805ba00-d666-11ea-9145-1eea0f9b05d2.png "")   
- Logistic function g(z)는 z가 0보다 크거나 같으면, 0.5보다 크거나 같게 됨.   
  ![4](https://user-images.githubusercontent.com/68726615/89259965-589e5080-d666-11ea-9267-45366397a446.png "")   
- Logistic function g의 input θ^TX 일 때,   
  ![5](https://user-images.githubusercontent.com/68726615/89259966-5936e700-d666-11ea-9a11-9e53e2349ff7.png "")   
- 결론적으로, 다음과 같이 말할 수 있음.   
  ![6](https://user-images.githubusercontent.com/68726615/89259968-5936e700-d666-11ea-87db-261ad589cf69.png "")   
- __Decision Boundary__ 는 y = 0과 y = 1을 나누는 경계선을 말하며, hypothesis function에 의해 만들어짐.   
  ![7](https://user-images.githubusercontent.com/68726615/89259970-59cf7d80-d666-11ea-85dc-c0524816b39b.png "")   
  ![](https://wikidocs.net/images/page/4288/logreg303.PNG "")    
  - E.g.   
    ![8](https://user-images.githubusercontent.com/68726615/89259972-59cf7d80-d666-11ea-9e08-e18931857420.png "")   
 - Decision Boundary는 꼭 Linear 할 필요가 없음. Non-linear decision boundary도 다음과 같이 만들 수 있음.   
  ![](https://wikidocs.net/images/page/4288/logreg305.PNG "")   
# Logistic Regression Model
## 5.4 Cost Function
- Logistic Regression에서 Linear Regression의 Cost function을 그대로 사용할 수 없음. 그대로 사용하게 되면, "non-convex"의 그래프가 되어, 많은 loccal optima들을 생성하기 때문.
- 대신에 아래의 Cost function을 사용함.   
  ![9](https://user-images.githubusercontent.com/68726615/89259974-5a681400-d666-11ea-9aff-203c6cdbcd65.png "")   
- 아래 그래프와 같이 y = 1일 때와 y = 0일 때 서로 다른 함수를 따르게 됨.
  ![](https://wikidocs.net/images/page/4289/logreg403.PNG "")   
- y = 1일 때, hypothesis function이 1이면 Cost function의 결과는 0이 됨. hypothesis가 0에 가까워질수록, Cost function의 값이 무한에 가까워짐.
- y = 0일 때, hypothesis function이 0이면 Cost function의 결과는 0이 됨. hypothesis가 1에 가까워질수록, Cost function의 값이 무한에 가까워짐.   
  ![10](https://user-images.githubusercontent.com/68726615/89259976-5a681400-d666-11ea-87df-94b5d68004b6.png "")   
## 5.5 Simplified Cost Function and Gradient Descent
- Cost function의 두 가지 조건을 하나의 식으로 표현할 수 있음.   
  ![11](https://user-images.githubusercontent.com/68726615/89259978-5b00aa80-d666-11ea-936c-0a01154de7d6.png "")   
- Entire cost function:   
  ![11](https://user-images.githubusercontent.com/68726615/89259980-5b00aa80-d666-11ea-8bb4-8dd52874d926.png "")   
- gradient descent를 통해 위 cost function을 최소화하는 parameter θ를 찾을 수 있음.   
  ![12](https://user-images.githubusercontent.com/68726615/89259983-5b994100-d666-11ea-95ef-728d179cf731.png "")   
  ![13](https://user-images.githubusercontent.com/68726615/89259984-5b994100-d666-11ea-8e60-e7fd76eaafc0.png "")   
- 이 수식만 보면 linear regression의 gradient descent와 동일하지만, hypothesis function이 다름.   
  ![14](https://user-images.githubusercontent.com/68726615/89259987-5c31d780-d666-11ea-9fa5-94db6ecbf3f6.png "")   
## 5.6 Advanced Optimization
- Gradient descent 외에도 learning rate α 구하기 위한 Optimization algorithm들이 있음.
  - Gradient descent
  - Conjugate gradient
  - BFGS
  - L-BFGS
- 위 알고리즘들은 learning rate α를 자동으로 선택하며 gradient descent보다 대체적으로 빠름. 하지만 복잡하다는 단점이 있음.
# Multiclass Classficication
## 5.7 Muliticlass Classification: One-vs-all
- Multiclass classification의 예
  - Email foldering/tagging: Work, Friends, Family, Hobby
  - Medical diagrams: Not ill, cold, flu
  - Weather: Sunny, Cloudy, Rain, Snow   
  ![](https://wikidocs.net/images/page/4291/logreg702.PNG "")
### One-vs-All (One-vs-Rest)
- 각 class 별로 해당 class vs 나머지 class들로 binary decision을 내리도록 만든 후, hypothesis function 값이 가장 큰 것을 선택하게 함.   
  ![](https://wikidocs.net/images/page/4291/logreg703.PNG "")   
  ![15](https://user-images.githubusercontent.com/68726615/89259957-563bf680-d666-11ea-84c1-c5cd733f77d7.png "")
