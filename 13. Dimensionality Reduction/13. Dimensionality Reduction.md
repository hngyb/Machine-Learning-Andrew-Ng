# 13. Dimensionality Reduction
# Motivation
## 13.1 Motivation 1: Data Compression
- Correlation이 높은 feature를 찾고, 그래프를 그려서 두 feature를 동시에 대표할 수 있는 새로운 line을 찾아냄으로써 feature dimension을 줄일 수 있음.
- Dimensionality reduction은 컴퓨터 메모리에 저장되는 데이터 양을 줄이고, learning algorithm 속도를 높일 수 있음.
- Reduce data from 2D to 1D   
    ![](https://wikidocs.net/images/page/4669/dim101.PNG)   
## 13.2 Motivation 2: Visualization
- 고차원 데이터는 visualization이 어려움.
- 고차원 데이터를 2D or 3D로 줄이면 효과적인 visualization이 가능함. (기존 feature를 효과적으로 요약해주는 새로운 feature z1, z1를 찾음)
# Principal Component Analysis
## 13.3 Principal Component Analysis Problem Formulation
- Principal Component Analysis(PCA)는 가장 널리 이용되는 dimensionality reduction algorithm임.   
    ![](https://wikidocs.net/images/page/4870/dim201.PNG)   
- Reduce from 2D to 1D: Data를 project했을 때, 그 projection error를 최소로 만드는 벡터를 찾음.
- Reduce from n-dim to k-dim: Data를 project했을 때, 그 project error를 최소로 만드는 k개 벡터를 찾음.
### Relationship to Linear Regression
- PCA와 linear regression은 비슷해보이지만 전혀 다름.
    - linear regression에서는 각 데이터점과 prediction line 간의 __squared error__ (vertical distance)를 최소화함.
    - PCA에서는 각 데이터점과 projection line 간의 __shortest distance__ (or orthogonal distance)를 최소화함.   
    ![](https://wikidocs.net/images/page/4870/dim202.PNG)   
    - linear regression에서는 y를 예측하기 위함이지만, PCA에서는 y값이 존재하지 않고 feature x들 간의 가장 공통적인 데이터셋을 찾는 것임.
## 13.4 Principal Component Analysis Algorithm
### Data Preprocessing
- PCA 하기 전에 data preprocessing이 필요함. (feature scaling/mean normalizawtion)   
    ![1](https://user-images.githubusercontent.com/68726615/92325962-90dfe700-f089-11ea-8551-9f70b0b31f25.png)   
### Algorithm
1. Compute "covariance matrix" (공분산 행렬)   
    ![](https://wikidocs.net/images/page/4870/dim3021.png)   
    - 여기서 x^(i)는 n*1 vector이고, (x^(i))^T는 1*n vector이며 X는 m*n matrix임. 이 행렬의 곱 연산은 n*n 행렬이 됨.
2. Compute "eigenvectors" (고유 벡터) of matrix Σ   
    ![](https://wikidocs.net/images/page/4870/dim3022.png)   
    - SVD는 singular value decomposition을 의미함.
    - SVD 과정을 통해 covariance matrix의 U ∈ ℝ^(n×n) 행렬을 구하는 것이 목적임.
    - U는 u^(1), ..., u^(n)으로 이루어져 있고, 이 벡터들이 우리가 원하는 것.   
    ![](https://wikidocs.net/images/page/4870/dim3023.png)   
3. Take the first k columns of the U matrix and compute z
    - 이제 U 행렬의 첫번째 k개 열을 변수 'Ureduce'에 assign함. 이것은 n*k matrix가 됨.
    - 이 행렬로 z를 계산함.   
    ![2](https://user-images.githubusercontent.com/68726615/92325961-90475080-f089-11ea-9d31-3bd9b16f95ae.png)   
# Applying PCA
## 13.5 Reconstruction from Compressed Representation
- PCA로 압축한 데이터를 다시 원래대로 돌리는 방법.
- 1D to 2D로 확장려면, z ∈ ℝ → x ∈ ℝ^2
- 이 과정은 다음과 같은 방정식으로 나타낼 수 있음.   
    ![3](https://user-images.githubusercontent.com/68726615/92325960-90475080-f089-11ea-8d34-52a012388990.png)   
- 이렇게 해서 얻어지는 데이터는 원래 데이터의 근사값임.   
    ![](https://wikidocs.net/images/page/4871/dim401.PNG)   
## 13.6 Choosing the Number of Principal Components
- Number of principal component = k (목표로 하는 차원의 수)
- 다음 공식에 따라 k를 선택함.   
    ![4](https://user-images.githubusercontent.com/68726615/92325959-8faeba00-f089-11ea-903e-427740793cae.png)   
- 이와 같은 조건을 만족할 때 variance의 99%가 보존됨.
- 필요에 따라 위 수식에서 0.01 대신 0.05, 0.10 등 다른 값을 사용할 수도 있음.
- 그 경우 보존되는 variance는 95%, 90% 등임.
### Algorithm (Not recommended)   
  ![](https://wikidocs.net/images/page/4872/dim501.PNG)   
- k를 점차 늘려가며 PCA를 수행하고 위 ratio가 0.01보다 작은지 확인하는 방식은 굉장히 비효율적임.
### Algorithm (Using SVD)
- SVD (Singular Value Decomposition)을 이용하면 k를 구하는 것이 용이함.
- 먼저 covariance matrix Sigma를 구한 다음, Sigma에 대하여 SVD를 함.
- [U, S, V] = svd(Sigma)
- matrix S를 이용하면 어느 k에서 retained varaince가 99% 이상이 되는지 확인하는 과정을 더 효과적으로 할 수 있음.   
    ![](https://wikidocs.net/images/page/4872/dim5021.png)   
- 즉, 다음 부등식을 만족하는 가장 작은 k를 찾음.   
    ![5](https://user-images.githubusercontent.com/68726615/92325958-8f162380-f089-11ea-868f-32df0c34114e.png)   
## 13.7 Advice for Applying PCA
### Supervised learning speedup
- PCA를 이용해서 feature dimension을 줄이면 훨씬 빠르게 연산이 가능함.
1. Extract inputs  
    ![](https://wikidocs.net/images/page/4873/dim602.PNG)   
2. New training set   
    ![](https://wikidocs.net/images/page/4873/dim603.PNG)   
### Application of PCA
- Compression
    - Reduce memory/disk needed to stor data
    - Speed up learning algorithm   
    -> Choose k by % of variance retain
- Visualization   
    -> k = 2 or k = 3
### Bad use of PCA: To prevent overfitting
- feature dimension을 줄임으로써 overfitting을 피하는데 어느정도 동작할 수 있지만, 좋은 방법은 아님.
- PCA는 정보의 양을 줄이고, y에 상관없이 동작하기 때문에 꼭 필요한 정보를 잃을 수 있음.
- PCA 대신에 regularization을 사용하는 것이 더 나음.
### PCA is sometimes used where it shouldn't be
- ML system을 설계할 때 PCA를 처음부터 포함하는 경우가 있음.   
    ![](https://wikidocs.net/images/page/4873/dim704.PNG)   
- 그러나 PCA를 구현하기 전에, 먼저 원래 데이터 x를 이용하여 시도해보는 것이 좋음.
- 원하는대로 동작하지 않으면 그때가서 PCA와 z를 이용하는 것을 고려.