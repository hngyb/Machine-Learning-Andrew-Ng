# 11. Support Vector Machines
# Large Margin Classification
## 11.1 Optimization Objective
### Logistic regression의 hypothesis function   
  ![](https://wikidocs.net/images/page/4268/svm101.PNG "")   
    - y = 1일 때 θ^(T)x≫0로 만들어주고, y = 0일 때 θ^(T)x≪0으로 만들어주는 θ를 찾고자 했음.
### Alternative view of logistic regression   
  ![](https://wikidocs.net/images/page/4268/svm102.PNG "")   
- Logistic regression은 log를 이용하여 cost function을 정의했음.
- SVM은 piecewise-linear한 함수를 대신 이용함.
### Support vector machine   
  ![](https://wikidocs.net/images/page/4268/svm103.PNG "")   
- Logistic regression cost function에 새롭게 정의한 cost1()과 cost2()를 대입함.
- "1/m"은 최솟값을 구하는데 영향을 주지 않으므로 지움.
- regularization trade-off 를 조정하는 계수 λ대신 C=1/λ를 사용해 trade-off를 조정함.
### SVM hypothesis
- SVM의 hypothesis는 y가 1이거나 0일 "확률"로 해석하지 않음.
- Hypothesis function의 output 자체가 0 또는 1임.   
    ![](https://wikidocs.net/images/page/4268/svm201.PNG "")   
## 11.2 Large Margin Intuition
### SVM Decision Boundary   
  ![](https://wikidocs.net/images/page/4280/svm203.PNG "")   
- 0이 아닌 +1과 -1을 기준으로 classification을 함.
- 이 부분이 SVM이 margin을 최대화하도록 하는 역할을 함.
- 마진은 하나의 데이터포인트(Support Vector)와 판별경계(Hyperplane)사이의 거리를 말함.
- 더 정확히는 각각의 클래스의 데이터 벡터들로 부터 주어진 판별 경계까지의 거리 중 가장 짧은 것을 말함.
- SVM에서는 이 마진이 클수록 분별을 잘하는 분류기임.   
    ![](https://wikidocs.net/images/page/4280/svm207.PNG "")   
- Decision boundary의 기울기가 달라도 주어진 데이터를 2개 class로 나눌 수 있음.
- 그러나 표시된 선이 선택된 이유는 그 선이 두 class 사이의 margin을 최대화하기 때문임.   
    ![](https://wikidocs.net/images/page/4280/svm204.PNG "")   
- 괄호 안의 수식이 0이 되는 부분이 decision boundary가 됨. 그 부분에 0을 넣고 나면 최적화 문제는 다음과 같이 단순화됨.   
    ![](https://wikidocs.net/images/page/4280/svm206.PNG "")   
- y(i)=1일 때에는 θTx(i)≥+1이 되게 하고 y(i)=0일 때에는 θTx(i)≤−1이 되게 하는 것.
### Large margin classifier in presence of outliers   
  ![](https://wikidocs.net/images/page/4280/svm301.PNG "")   
- C가 아주 크면 outlier에 민감해짐.
- 녹색 선 보다는 회색 선이 prediction에 유리함.
- 그러므로 너무 크지 않은 C 값을 적절히 선택하는 것이 중요함.
## 11.3 Mathematics Behind Large Margin Classification
### Vector Inner Product   
  ![](https://wikidocs.net/images/page/4280/svm302.PNG "")   
- 두 벡터의 내적 u^(T)v는, 벡터 v를 u에 project한 벡터의 길이 p에 벡터 u의 길이를 곱한 것임.
- 이 때 p는 scalar이지만 음수가 될 수도 있음.
### SVM Decision Boundary
- SVM의 decision boundary를 찾는 것은 다음의 optimization problem을 푸는 것임.   
    ![](https://wikidocs.net/images/page/4280/svm303.PNG "")   
- 문제를 단순화하기 위해 bias θ0 = 0으로 놓고, 2개의 feature만 있다고 가정함.   
    ![](https://wikidocs.net/images/page/4280/svm304.PNG "")   
    ![](https://wikidocs.net/images/page/4280/svm305.PNG "")   
- 내가 찾은 θ에 해당하는 p값이 작다면 p*||θ||≥+1 을 만족시키는 θ의 길이는 커져야 함.
- p값이 작으면 ||θ||의 값은 작아질 수 있음.
- 가장 작은 ||θ||를 찾는 것이 목적임. 그러므로 최종 결과는 최대 margin의 decision boundary가 됨.
# Kernels
## 11.4 Kernels 1
### Non-linear Decision Boundary   
  ![](https://wikidocs.net/images/page/4282/svm401.PNG "")   
- x1,x2,... 와 같은 feature들을 좀 더 일반적인 형태로 표현하기 위해 f1, f2,...로 고쳐 씀.
### Kernel   
  ![](https://wikidocs.net/images/page/4282/svm402.PNG "")   
- feature space에 임의의 landmark l(1), l(2), l(3)가  있음.
- 새로운 feature f 들은 원래 feature가 각 landmark에 얼마나 가까운가를 나타내는 것으러 정의함.
- 이러한 함수를 kernel이라고 함.
- 이 예에서는 Gaussian 함수를 사용함.
- 이 함수는 landmark에 가까울수록 큰 값을, 멀수록 작은 값을 출력함.   
    ![](https://wikidocs.net/images/page/4282/svm403.PNG "")   
- example x가 첫번째 landmark에 가깝다면  f1 ≈ 1이 되고, 그 landmark에서 멀다면  f1 ≈ 0이 됨.   
    ![](https://wikidocs.net/images/page/4282/svm404.PNG "")   
- SVM: Predict "1" when θ0+θ1f1+θ2f2+θ3f3≥0
- θ0=−0.5,θ1=1,θ2=1,θ3=0이라고 하면, x는 l(1)에 가깝고 l(2),l(3)에 멀기 때문에 f1≈1,f2≈0,f3≈0이 됨.
- θ0+θ1⋅1+θ2⋅0+θ3⋅0=0.5≥0 이므로 해당하는 점은 y=1 영역에 포함됨.
- 이와 같은 방법으로 여러 위치의 x를 입력하여 어떤 y값이 출력되는지 계산하면 decision boundary를 구할 수 있음.
- 이 예시에서의 decision boundary는 노란색 선, 즉 non-linear decision boundary가 됨.
## 11.5 Kernels 2
### Choosing the Landmarks
- 한가지 방법은 landmark를 각 training example과 같은 위치에 놓는 것임.
  ![](https://wikidocs.net/images/page/4282/svm502.PNG "")   
  ![](https://wikidocs.net/images/page/4282/svm503.PNG "")   
  ![](https://wikidocs.net/images/page/4282/svm504.PNG "")   
### SVM with Kernels   
  ![](https://wikidocs.net/images/page/4282/svm506.PNG "")   
- kerenl을 도입한 새 cost function은 단순히 x 자리에 f 를 넣으면 됨.
- SVM이 아니라도 kernel 개념을 도입할 수 있지만 SVM 외의 classifier에서는 연산속도가 매우 느려짐. 그래서 kernel은 거의 SVM에서만 사용됨.
### SVM Parameters   
  ![](https://wikidocs.net/images/page/4283/svm507.PNG "")   
- Large C = Small λ --> Overfit
- Small C = Large λ --> Underfit 
# SVMs in Practive
## 11.6 Using An SVM
- SVM을 직접 구현하기보단 기존에 구축된 package를 사용하는 편이 나음.
- 명시해야할 것은 1)Parameter C 와 2)Kernel(similarity function)을 선택하는 것임.
- Gaussian kerenl을 사용하기 전에 feature scalingd을 해야함.
### Other choices of kernel
- 모든 similarity function이 유효한 kernel은 아님.
- "Mercer's Theorem"이라는 조건을 만족해야 SVM package의 optimization이 정상적으로 작동함.
- 많은 off-the-shelf-kernel들이 사용가능함:
  - Polynomial kernel, String kernel, chi-square kernel, histogram intersection kerenl, ...
### Multi-Class Classification
- 많은 SVM packages들이 이미 built-in-multi-class classification 기능을 갖추고 있음.
- one-vs-all method를 사용해도 됨. (Train K SVMs, one to distinguish y = i from the rest, for i = 1, 2, ..., K)
### Logistic regression vs. SVMs   
  ![](https://wikidocs.net/images/page/4283/svm603.PNG "")   
- Logistic regression과 SVM without a kernel (linear kernel)은 거의 비슷한 알고리즘으로, 거의 동일한 결과를 가져다줌.
- Neural network가 대부분의 경우 잘 작동하나, train하는데 시간이 걸림.