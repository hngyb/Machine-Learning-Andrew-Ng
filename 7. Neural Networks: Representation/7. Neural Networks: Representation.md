# 7. Neural Networks: Representation
# Motivations
## 7.1 Non-linear Hypotheses
- linear regression이나 logistic regression으로는 복잡한 데이터를 다루는 데 한계가 있음.
- logistic regression을 이용해서 non-linear boundary를 만들 수도 있지만, feature의 개수가 많아지만 한계가 있음.   
    ![](https://wikidocs.net/images/page/4259/nn003.PNG "")   
- 이렇게 되면, feature의 개수가 기하급수적으로 증가해서 연산량이 늘어나고, overfitting의 가능성이 높아짐.
## 7.2 Neurons and the Brain
### Neural Networks
- 두뇌(뇌의 신경세포)를 모방하기 위한 알고리즘.
- 80년대~90년대 초반에 널리 사용되다가 90년대 후반에 사라짐.
- 최근 하드웨어 발전에 힘입어, 현재 다양한 분야에서 최첨단 기술로 사용됨.
### The "one learning algorithm" hypothesis   
  ![](https://wikidocs.net/images/page/4259/nn004.PNG "")   
- 뇌가 학습하는 알고리즘은 단 한 가지 뿐.
# Neural Networks
## 7.3 Model Representation 1
### Neurons in the brain   
  ![](https://wikidocs.net/images/page/4260/nn201.PNG "")   
- 신경세포를 단순화해 하나의 computational unit으로 생각.
- Dendrite = "input wires"
- Axon = "output wires"
- 입력단에서 전기 신호 (spikes)를 입력으로 받아 처리하여 출력단으로 전달하는 역할.
### Neuron model: Logistic unit   
  ![](https://wikidocs.net/images/page/4260/nn202.PNG "")   
- Dendrite은 input feature x1, ..., xn
- axon은 hypothesis functiond의 output으로 표현.
- x0은 "bias unit"으로 항상 1값을 가지므로 굳이 나타내지 않기도 함.
- Sigmoid (logistic) activation function이라고 부르며, θ는 "weights"라고 하기도 함.
### Neural Network   
  ![](https://wikidocs.net/images/page/4260/nn203.PNG "")
- 첫번째 layer를 __"input layer"__, 마지막 layer를 __"output layer"__, input과 output 중간의 layer들은 __"hidden layer"__ 라고 함.
- "hidden layer"의 node들을 __"activation unit"__ 이라 부름.   
    ![1](https://user-images.githubusercontent.com/68726615/89983488-94fb2d80-dcb2-11ea-94b8-83ee3fc5cd64.png "")   
    ![](https://wikidocs.net/images/page/4260/nn204.PNG "")   
- 각 activation node는 3*4 matrix의 parameter에 의해 계산됨.
- 각 레이어는 고유한 weight matrix Θ^(j)를 가짐.
- Θ^(j)의 dimension은   
    ![2](https://user-images.githubusercontent.com/68726615/89983486-94fb2d80-dcb2-11ea-945c-5bb9c31830bc.png "")   
- "+1" 은 "bias node" x0과 Θ0^(j) 때문에 생김. output node는 bias node를 포함하지 않는 반면, input에는 포함됨.
## 7.4 Model Representation 2
### Forward propagation: Vectored Implementaion
  ![](https://wikidocs.net/images/page/4260/nn302.PNG "")
- 입력값들을 벡터로 표현하면,   
  ![3](https://user-images.githubusercontent.com/68726615/89983484-94629700-dcb2-11ea-98af-9e6d0759c58f.png "")   
- x = a^(1)이라 나타내면, layer 2에서   
  ![4](https://user-images.githubusercontent.com/68726615/89983483-94629700-dcb2-11ea-9fa2-3195deaccd22.png "")   
  ![5](https://user-images.githubusercontent.com/68726615/89983482-93ca0080-dcb2-11ea-8ba9-56ade698d47a.png "")   
- bias unit a0^(2) = 1을 추가하면, a^(2) ∈ R^4   
  ![6](https://user-images.githubusercontent.com/68726615/89983480-93316a00-dcb2-11ea-9ee5-fc7894fd8505.png "")   
- 일반화하면,   
  ![7](https://user-images.githubusercontent.com/68726615/89983479-93316a00-dcb2-11ea-929f-8282256f9dfd.png "")   
- 최종 결과는,   
  ![8](https://user-images.githubusercontent.com/68726615/89983478-9298d380-dcb2-11ea-9eb6-37da8daa61d8.png "")   
- 이 예시에서는,   
  ![9](https://user-images.githubusercontent.com/68726615/89983476-9298d380-dcb2-11ea-8b9a-4313f13989f9.png "")   
## 7.5 Examples and Intuitions 1
### Simple example: AND
- x1 AND x2를 예측하는 neural net, AND는 x1과 x2가 모두 참일 때만 1을 출력하는 논리 연산자임.   
  ![10](https://user-images.githubusercontent.com/68726615/89983474-92003d00-dcb2-11ea-9f0c-86bb7975a803.png "")   
- x0은 bias이며 항상 1값을 가짐.
- theta matrix는 다음과 같음.   
  ![11](https://user-images.githubusercontent.com/68726615/89983471-9167a680-dcb2-11ea-94ad-1dd16e198cf7.png "")   
- truth table를 통해 hypothesis의 값을 확인해보면,   
  ![](https://user-images.githubusercontent.com/68726615/93430443-a660d680-f8fd-11ea-95ad-a13093382b42.png)   
  ![12](https://user-images.githubusercontent.com/68726615/89983467-8f9de300-dcb2-11ea-8787-0863d2f3f1ca.png "")   
- 다른 논리 연산자들도 이와 같은 방법으로 neural net을 통해 구현할 수 있음
## 7.6 Examples and Intuitions 2
### XNOR
- hidden layer 2개를 사용하여 XNOR 연산자를 만들 수 있음.   
  ![](https://user-images.githubusercontent.com/68726615/93430642-f049bc80-f8fd-11ea-8ce2-e4f2f0432100.png)   
## 7.7 Multiclass Classification
- hypothesis 결과값을 벡터화하여 multiple class를 표현할 수 있음.
- One-vs-all 를 활용한 다중분류의 예시   
  ![](https://user-images.githubusercontent.com/68726615/93430629-ec1d9f00-f8fd-11ea-80c9-22b881042d2c.png)   
  - resulting class들을 다음과 같이 표현할 수 있음.   
    ![](https://user-images.githubusercontent.com/68726615/93430640-efb12600-f8fd-11ea-97ac-bdf8dbbdf7d3.png)   
  - Each y^(i) represents a different image corresponding to either a car, pedestrian, truck, or motorcycle.   
    ![](https://user-images.githubusercontent.com/68726615/93430646-f17ae980-f8fd-11ea-9dc6-3312a3060490.png)
    