# 6. Regularization
# Solving the Problem of Overfitting
## 6.1 The Problem of Overfitting   
  ![](https://user-images.githubusercontent.com/68726615/93430119-220e5380-f8fd-11ea-868d-d95c30bb8419.png)
- left: "underfit", "High bias"
    - Hypothesis function h maps poorly to the trend of the data.
- middle: "Just right"
- right: "overfit", "High variance"
    - Hypothesis function h fits the available data but does not generalize well to predict unnecessary curves and angles unrelated to the data. 
- __Overfitting__: training data에 지나치게 (over) fit 되어 일반적인 추세를 표현하지 못하는 문제.
### Addressing Overfitting
- overfitting을 피하기 위한 Options:
    1. Reduce number of features.
        - 사용할 feature를 수동으로 선택.
        - Model selection algorithm
    2. Regularization
        - 모든 feature들이 조금씩은 예측에 도움이 되는 경우, feature 개수를 유지하는 방법.
        - 모든 feature들을 유지하되, parameters θj의 크기를 줄임.
        - 많은 feature가 y를 예측하는 데 조금씩 기여하는 경우 유용함.
## 6.2 Cost Function
### Intuition
![](https://wikidocs.net/images/page/4329/rg201.PNG "")
-  θ3, θ4의 값이 커질수록 penalize.   
    ![0](https://user-images.githubusercontent.com/68726615/89644180-52240900-d8f2-11ea-85f3-3e5f78ca23a4.png "")   
- 4차함수 모델을 사용하더라도 2차함수 모델에 거의 근접하게 됨.
### Regularization
- Small values for parameters θ0,θ1,θ2,...,θn.
    - "Simpler" hypothesis. Hypothesis function이 단순해짐.
    - Less prone to overfitting. Overfitting 가능성이 적어짐.   
    ![1](https://user-images.githubusercontent.com/68726615/89644178-50f2dc00-d8f2-11ea-8c3c-dbf1c9b36d37.png "")   
 - 일종의 penalty λ를 곱해주는 것으로 parameter의 크기를 작게 유지할 수 있음.
 - 그러나 λ가 너무 커지면, __θj ≈ 0__ 이 되어 hθ(x) = θ0. underfit 문제가 발생함.
 - 적절한 λ 선택이 중요함.
## 6.3 Regularized Linear Regression
- Linear Regression에 Regularization 적용.
### Gradient Descent
   ![](https://wikidocs.net/images/page/4330/rg303.PNG "")
- θ0는 regularization 안함. 
- 1 - α*(λ/m) < 1, reducing the value of θj.
### Normal Equation
   ![](https://wikidocs.net/images/page/4330/rg304.PNG "")
### Non-Invertibility
   ![](https://wikidocs.net/images/page/4330/rg305.PNG "")
- example의 개수가 feature 개수보다 작을 때 발생하는 non-invertibility 문제도 해결.
- X^T * X + λ⋅L becomes invertible.
## 6.4 Regularized Logistic Regression
### Cost Function
   ![2](https://user-images.githubusercontent.com/68726615/89644170-4fc1af00-d8f2-11ea-899f-2f579a1915a0.png "")
### Gradient Descent
   ![](https://wikidocs.net/images/page/4331/rg402.PNG "")
