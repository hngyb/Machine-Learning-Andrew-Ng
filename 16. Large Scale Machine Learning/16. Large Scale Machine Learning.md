# 16. Large Scale Machine Learning
# Gradient Descent with Large Datasets
## 16.1 Learning With Large Datasets
- It's not who has the best algorithm that wins. It's who has the most data.
- 앞으로 대량의 데이터를 이용한 학습은 어떻게 하는 것이 효율적인지 알아봄.
## 16.2 Stochastic Gradient Descent  
  ![](https://wikidocs.net/images/page/7178/102.PNG)   
- high variance인 경우에는 큰 dataset이 도움이 되지 않음.
- high bias인 경우에는 데이터를 추가하는 것이 도움을 주지 않음.
- 일반적인 (batch) Gradient descent:   
    ![1](https://user-images.githubusercontent.com/68726615/93321573-4a3e7980-f84d-11ea-99ec-e666882f6914.png)   
- m = 100,000,000이라고 하면, 1번 iteration 돌 때마다 100,000,000 번의 계산을 해야하기 때문에 Cost가 큼.
- Stochastic gradient descent는 batch gradient descent를 대신할 수 있는 방법이며, 큰 dataset를 학습할 때 효율적임.
### Linear Regression with Gradient Descent   
  ![2](https://user-images.githubusercontent.com/68726615/93321571-49a5e300-f84d-11ea-9e93-1f77c75a4360.png)   
- m이 크면 매 iteration마다 계산량이 크고 메모리를 많이 잡아먹는 문제가 생김.
### Batch Gradient Descent vs. Stochastic Gradient Descent   
  ![3](https://user-images.githubusercontent.com/68726615/93321568-4874b600-f84d-11ea-8606-31ce628feab7.png)   
## 16.3 Mini-Batch Gradient Descent
- Batch gradient descent: Use all m examples in each iteration.
- Stochastic gradient descent: Use 1 example in each iteration.
- Mini-batch gradient descent: Use b examples in each iteration (b = mini-batch size)   
  ![](https://wikidocs.net/images/page/7178/301.PNG)   
  ![](https://wikidocs.net/images/page/7178/302.PNG)   
## 16.4 Stochastic Gradient Descent Convergence
- To make sure its convergence.
- To choose the learning rate α.
- Stochastic Gradient Descent:
  - During learning, compute cost(θ,(x(i),y(i))) before updating θ using (x(i),y(i)).
  - Every 1000 iterations (say), plot cost(θ,(x(i),y(i))) averaged over the last 1,000 examples processed by algorithm.   
  ![](https://wikidocs.net/images/page/7178/401.PNG)   
# Advanced Topics
## 16.5 Online Learning
- 한 택배 회사 웹사이트에서 출발지와 배송지를 입력하면 배송비 견적을 제시해줌.
- 사용자들은 해당 택배 서비스를 이용하기로 결정할 수도 있고 (y=1) 아닐 수도 있음 (y=0).
- Feature x는 사용자 정보, 출발지, 배송지, 그리고 견적가 등이 될 수 있음. 이 때, 최적의 가격을 알아내기 위해 p(y=1|x;θ)를 구하고자 함.
- Logistic regression을 이용하면 다음과 같음.   
  ![](https://wikidocs.net/images/page/7179/501.PNG)   
- 일반적인 logistic regression과 다른 점이라면 하나의 example마다 θ를 업데이트하고 한 번 사용한 example은 버린다는 점.
- 이 방법을 사용하여 변화하는 사용자 성향에 맞춰 최적화할 수 있음.
### Other Online Learning example
- Product search
- Choosing special offers to show user
- Customized selection of news articles
- Product recommendation
- Online learning은 stochastic gradient descent와 비슷하지만, fixed set이 아닌 continous stream으로 data를 받으며 각 example을 한 번 쓰고 버린다는 점이 다름.
## 16.6 Map Reduce and Data Parallelism
### Map Reduce   
- Batch gradient descent, m = 400:
  ![](https://wikidocs.net/images/page/7179/701.PNG)   
### Map-reduce and summation over the training set
- Many learning algorithms can be expressed as comuting sums of functions over the training set
- E.g. for advanced optimization, with logistic regression, need:   
  ![4](https://user-images.githubusercontent.com/68726615/93321564-46125c00-f84d-11ea-9fbc-57dd5cb8ac30.png)   
### Multi-Core Machines   
  ![](https://wikidocs.net/images/page/7179/702.PNG)