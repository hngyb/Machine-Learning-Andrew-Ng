# 4. Linear Regression with Multiple Variables

# Multivariate Linear Regression

## 4.1 Multiple Features

- **다변량 선형회귀(multivariate linear regression)**: 여러 개의 변수들을 가지고 있는 선형회귀.
- 집값 추정 문제를 다변량 선형회귀에 적용해보면,

    ![https://wikidocs.net/images/page/7639/muti101.PNG](https://wikidocs.net/images/page/7639/muti101.PNG "")   
    - **Notation**   
    ![Untitled](https://user-images.githubusercontent.com/68726615/88783759-2ace8d00-d1ca-11ea-9909-96568d8b25eb.png "")   

- **Hypothesis:**

    ![Untitled 1](https://user-images.githubusercontent.com/68726615/88783761-2b672380-d1ca-11ea-89a3-8018e3ea2ca4.png "")

- Notation의 편의를 위해 x0 = 1로 설정 후, x와 θ를 Vector로 표현하면, Matrix Multiplication을 통해 Hypothesis function을 아래와 같이 표현할 수 있음.

    ![https://wikidocs.net/images/page/7639/muti104.PNG](https://wikidocs.net/images/page/7639/muti104.PNG "")

    ![Untitled 2](https://user-images.githubusercontent.com/68726615/88783762-2b672380-d1ca-11ea-8e59-d2fc15b5797a.png "")

- 이것은 한 개의 training example에 대한 hypothesis function의 vectorization임.

## 4.2 Gradient Descent for Multiple Variables

- Hypothesis:

    ![Untitled 3](https://user-images.githubusercontent.com/68726615/88783765-2bffba00-d1ca-11ea-943b-1aa89169a3d8.png "")

- Parameters: θ0, θ1, ... , θn ⇒ **θ (n + 1 dimensional vector)**
- **Cost function**:

    ![Untitled 4](https://user-images.githubusercontent.com/68726615/88783767-2bffba00-d1ca-11ea-833f-c6bd8aebf3ae.png "")

- Gradient Descent 식은 동일한 형태를 가지지만, 'n' 개의 features들을 반복해야 함.

    ![Untitled 5](https://user-images.githubusercontent.com/68726615/88783768-2c985080-d1ca-11ea-89c1-2983fef518a5.png "")

    ![Untitled 6](https://user-images.githubusercontent.com/68726615/88783769-2c985080-d1ca-11ea-8991-d2c7050f934a.png "")

- 아래의 사진은 한 개의 변수를 가진 gradient descent 와 다변량 gradient descent를 비교한 것.

    ![](https://user-images.githubusercontent.com/68726615/93429465-2dad4a80-f8fc-11ea-9b6e-017c0bcc4e93.png)

## 4.3 Gradient Descent in Practice 1 - Feature Scaling

- features를 same scale로 만들면, gradient descent의 속도를 향상시킬 수 있음. (θ will descend quickly on small ranges and slowly on large ranges)
- E.g.

    ![Untitled 7](https://user-images.githubusercontent.com/68726615/88783746-273b0600-d1ca-11ea-97b2-325af1d2bbb1.png "")

- **Feature Scaling**: Get every feature into approximately a **-1 ≤ xi ≤ 1** range. (or -0.5 ≤ xi ≤ 0.5)
    - 범위를 완전히 동일하게 만들 필요는 없음. 범위를 너무 넓게나, 너무 좁게 만들지만 않으면 됨.
- **Mean normalization:** Replace xi with **xi - ui** to make features have approximately zero mean (Do not apply to x0  = 1).

    ![Untitled 8](https://user-images.githubusercontent.com/68726615/88783751-299d6000-d1ca-11ea-80cb-190291fe8901.png "")
    - **ui :** the **average** of all the values for feature (i) in training set.
    - **si** : **range** (**max - min** or **standard deviation**)
    - E.g. 집값의 범위가 100에서 2000이고, 평균값이 1000이면,

        ![Untitled 9](https://user-images.githubusercontent.com/68726615/88783754-299d6000-d1ca-11ea-830a-f233a021a56d.png "")

## 4.4 Gradient Descent in Practice 2 - Learning Rate

- **Debugging: How to make sure gradient descent is working correctly.**
    - x축에 iteration의 횟수, y축에 cost function 값을 놓고 plotting을 진행.
    - J(θ)는 매 iteration마다 감소해야함.

    ![https://wikidocs.net/images/page/7640/muti402.PNG](https://wikidocs.net/images/page/7640/muti402.PNG "")

    - **Automatic convergence test**: Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as 10^(−3).
- Gradient descent가 다음과 같이 제대로 작동하지 않는다면, **α 값을 줄여야 함.**

![https://wikidocs.net/images/page/7640/muti403.PNG](https://wikidocs.net/images/page/7640/muti403.PNG "")

- 충분히 작은 α값을 사용했다면, J(θ)는 매 iteration마다 감소함.
- 그러나, α값이 너무 작으면, 수렴하는 데 시간이 오래 걸릴 수 있음.
- α값을 선택하기 위해서, α값에 3씩 곱해보면서 여러 값을 시도.

![https://wikidocs.net/images/page/7640/muti502.PNG](https://wikidocs.net/images/page/7640/muti502.PNG "")

## 4.5 Features and Polynomial Regression

- 여러 개의 features를 하나로 합쳐서 새로운 feature로 만들 수 있음.
- E.g. Area = frontage * depth

    ![https://wikidocs.net/images/page/7641/fd.png](https://wikidocs.net/images/page/7641/fd.png "")

### Polynomial Regression

- Hypothesis function이 굳이 선형일 필요는 없다. 데이터에 가장 적합한 형태로 나타내면 됨.

![https://wikidocs.net/images/page/7641/muti602.PNG](https://wikidocs.net/images/page/7641/muti602.PNG "")

- 2차함수는 포물선이 최고점 이후에 감소하는 형태이므로, 집의 크기에 따른 집값을 표현하기에는 알맞지 못하다.
    - 3차함수 꼴로 나타내면,

        ![Untitled 10](https://user-images.githubusercontent.com/68726615/88783755-2a35f680-d1ca-11ea-98fc-b8363d676f60.png "")

        ![Untitled 11](https://user-images.githubusercontent.com/68726615/88783757-2a35f680-d1ca-11ea-97ab-9680fd0828c9.png "")
    - 제곱근 함수로도 나타낼 수 있음.

        ![Untitled 12](https://user-images.githubusercontent.com/68726615/88783758-2ace8d00-d1ca-11ea-941f-33611bcbf2da.png "")

- 이렇게 feature를 만들면 변수의 범위가 이전보다 훨씬 커지게 되거나 달라질 수 있으므로, **feature scaling**이 더욱 중요해짐.

# Computing Parameters Analytically
## 4.6 Normal Equation
- Normal equation: Method to sole for θ analytically.
- Gradient descent는 최적의 θ를 구하기 위해 iteration을 돌아야하지만, normal equation은 한 번에 최적의 값을 찾을 수 있음.
### Intuition
- __1D (θ∈R)__
    - Feature가 1개인 경우, J(θ)는 2차방정식
    - θ에 대하여 미분한 결과가 0이 되도록 설정
- __Multiple feature (θ∈Rn+1)__
    ![https://wikidocs.net/images/page/7642/muti606.PNG](https://wikidocs.net/images/page/7642/muti606.PNG "")
    
    - E.g.
    
        ![https://wikidocs.net/images/page/7642/muti607.PNG](https://wikidocs.net/images/page/7642/muti607.PNG "")
        ![https://wikidocs.net/images/page/7642/muti608.PNG](https://wikidocs.net/images/page/7642/muti608.PNG "")
        
    - m examples(x(1),y(1)),...,(x(m),y(m)); n features
    
        ![https://wikidocs.net/images/page/7642/muti609.PNG](https://wikidocs.net/images/page/7642/muti609.PNG "")
- Normal Equation은 Feature Scaling 을 할 필요 없음.
### Comparison of gradient descent and the normal equation
|__Gradient Descent__|__Normal Equation__|
|:-----:|:-----:|
|Need to choose alpha|No need to choose alpha|
|Needs many iterations|No need to iterate|
|O(kn^2)|O(n^3), need to calculate inverse of (X^T * X)|
|Works well when n is large|Slow if n is very large|

- Feature 개수 n이 10,000d 초과하면 Gradient descent로 실행.
## 4.7 Normal Equation Noninvertibility
- 흔치 않지만, Normal Equation을 할 때 (X^T * X)의 역행렬이 존재하지 않는다면?
- Common reasons:
    - Redundant features, where two features are very closely related(i.e. they are linearly dependent)
    - Too many features (e.g. m <= n). In this case, delete some feeatures or use "regularization"
- Solutions to the above problems incluse __deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.__
- Octave의 pinv()는 역행렬이 없어도 계산해 줌.

